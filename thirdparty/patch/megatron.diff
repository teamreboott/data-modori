diff --git a/megatron/__init__.py b/megatron/__init__.py
index aa99c06..6ccd17d 100644
--- a/megatron/__init__.py
+++ b/megatron/__init__.py
@@ -9,6 +9,7 @@ from .global_vars import get_signal_handler
 from .global_vars import update_num_microbatches
 from .global_vars import get_tokenizer
 from .global_vars import get_tensorboard_writer
+from .global_vars import get_wandb
 from .global_vars import get_adlr_autoresume
 from .global_vars import get_timers
 from .initialize  import initialize_megatron
diff --git a/megatron/arguments.py b/megatron/arguments.py
index 6cc1cc0..c9a941d 100644
--- a/megatron/arguments.py
+++ b/megatron/arguments.py
@@ -638,35 +638,42 @@ def _add_logging_args(parser):
                        '  max: report the max timing across all ranks'
                        '  minmax: report min and max timings across all ranks'
                        '  all: report timings of all ranks.')
-    group.add_argument('--tensorboard-log-interval', type=int, default=1,
-                       help='Report to tensorboard interval.')
+    group.add_argument('--tracker-log-interval', type=int, default=1,
+                       help='Report to trackers interval.')
     group.add_argument('--tensorboard-queue-size', type=int, default=1000,
                        help='Size of the tensorboard queue for pending events '
                        'and summaries before one of the ‘add’ calls forces a '
                        'flush to disk.')
-    group.add_argument('--log-timers-to-tensorboard', action='store_true',
-                       help='If set, write timers to tensorboard.')
-    group.add_argument('--log-batch-size-to-tensorboard', action='store_true',
-                       help='If set, write batch-size to tensorboard.')
-    group.add_argument('--no-log-learnig-rate-to-tensorboard',
+    group.add_argument('--log-timers-to-tracker', action='store_true',
+                       help='If set, write timers to trackers.')
+    group.add_argument('--log-batch-size-to-tracker', action='store_true',
+                       help='If set, write batch-size to trackers.')
+    group.add_argument('--no-log-learnig-rate-to-tracker',
                        action='store_false',
-                       help='Disable learning rate logging to tensorboard.',
-                       dest='log_learning_rate_to_tensorboard')
-    group.add_argument('--no-log-loss-scale-to-tensorboard',
+                       help='Disable learning rate logging to trackers.',
+                       dest='log_learning_rate_to_tracker')
+    group.add_argument('--no-log-loss-scale-to-tracker',
                        action='store_false',
-                       help='Disable loss-scale logging to tensorboard.',
-                       dest='log_loss_scale_to_tensorboard')
-    group.add_argument('--log-validation-ppl-to-tensorboard',
+                       help='Disable loss-scale logging to trackers.',
+                       dest='log_loss_scale_to_tracker')
+    group.add_argument('--log-validation-ppl-to-tracker',
                        action='store_true',
                        help='If set, write validation perplexity to '
-                       'tensorboard.')
-    group.add_argument('--log-memory-to-tensorboard',
+                       'trackers.')
+    group.add_argument('--log-memory-to-tracker',
                        action='store_true',
-                       help='Enable memory logging to tensorboard.')
-    group.add_argument('--log-world-size-to-tensorboard',
+                       help='Enable memory logging to trackers.')
+    group.add_argument('--log-world-size-to-tracker',
                        action='store_true',
-                       help='Enable world size logging to tensorboard.')
-
+                       help='Enable world size logging to trackers.')
+    group.add_argument('--wandb-project', type=str, default=None,
+                       help='Wandb project name')
+    group.add_argument('--wandb-group', type=str, default=None,
+                       help='Wandb group name')
+    group.add_argument('--wandb-master-name', type=str, default='master',
+                       help='The name of master node in wandb')
+    group.add_argument('--wandb-worker-name', type=str, default='worker',
+                       help='The name of worker node in wandb')
     return parser
 
 
@@ -706,7 +713,7 @@ def _add_regularization_args(parser):
 def _add_training_args(parser):
     group = parser.add_argument_group(title='training')
 
-    group.add_argument('--micro-batch-size', type=int, default=None,
+    group.add_argument('--micro-batch-size', type=int, default=1,
                        help='Batch size per model instance (local batch size). '
                        'Global batch size is local batch size times data '
                        'parallel size times number of micro batches.')
@@ -841,6 +848,8 @@ def _add_training_args(parser):
                        help='Disable fusing gradient accumulation to weight '
                        'gradient computation of linear layers',
                        dest='gradient_accumulation_fusion')
+    group.add_argument('--use-rmsnorm', action='store_true',
+                       help='Enable using RMSNorm instead of normal LayerNorm.')
     return parser
 
 
@@ -925,6 +934,9 @@ def _add_checkpointing_args(parser):
                        help='Do not load optimizer when loading checkpoint.')
     group.add_argument('--no-load-rng', action='store_true', default=None,
                        help='Do not load rng state when loading checkpoint.')
+    group.add_argument("--load-iteration", type=int, default=0,
+                       help='Load the checkpoint of this iteration, '
+                       'set 0 to load the latest checkpoint.')
     group.add_argument('--finetune', action='store_true',
                        help='Load model for finetuning. Do not load optimizer '
                        'or rng state from checkpoint and set iteration to 0. '
diff --git a/megatron/checkpointing.py b/megatron/checkpointing.py
index e88b585..6dc8c5b 100644
--- a/megatron/checkpointing.py
+++ b/megatron/checkpointing.py
@@ -357,27 +357,30 @@ def fix_query_key_value_ordering(model, checkpoint_version):
                      " checkpoint version {}".format(checkpoint_version))
 
 
-def _load_base_checkpoint(load_dir, rank0=False):
+def _load_base_checkpoint(load_dir, load_iteration=0, rank0=False):
     """ Load the base state_dict from the given directory
 
     If rank0 is true, just loads rank 0 checkpoint, ignoring arguments.
     """
+    if load_iteration != 0:
+        iteration = load_iteration
+        release = False
+    else:
+        # Read the tracker file and set the iteration.
+        tracker_filename = get_checkpoint_tracker_filename(load_dir)
 
-    # Read the tracker file and set the iteration.
-    tracker_filename = get_checkpoint_tracker_filename(load_dir)
-
-    # If no tracker file, return nothing
-    if not os.path.isfile(tracker_filename):
-        if not rank0:
-            print_rank_0('WARNING: could not find the metadata file {} '.format(
-                tracker_filename))
-            print_rank_0('    will not load any checkpoints and will start from '
-                         'random')
-        return None, "", False
+        # If no tracker file, return nothing
+        if not os.path.isfile(tracker_filename):
+            if not rank0:
+                print_rank_0('WARNING: could not find the metadata file {} '.format(
+                    tracker_filename))
+                print_rank_0('    will not load any checkpoints and will start from '
+                            'random')
+            return None, "", False
 
-    # Otherwise, read the tracker file and either set the iteration or
-    # mark it as a release checkpoint.
-    iteration, release = read_metadata(tracker_filename)
+        # Otherwise, read the tracker file and either set the iteration or
+        # mark it as a release checkpoint.
+        iteration, release = read_metadata(tracker_filename)
 
     # Checkpoint.
     if rank0:
@@ -431,7 +434,7 @@ def load_args_from_checkpoint(args, load_arg='load'):
         print_rank_0('No load directory specified, using provided arguments.')
         return args
 
-    state_dict, checkpoint_name, release = _load_base_checkpoint(load_dir, rank0=True)
+    state_dict, checkpoint_name, release = _load_base_checkpoint(load_dir, load_iteration=args.load_iteration, rank0=True)
 
     # Args.
     if not state_dict:
@@ -482,6 +485,8 @@ def load_args_from_checkpoint(args, load_arg='load'):
     _set_arg('apply_layernorm_1p', force=True)
     _set_arg('tokenizer_type')
     _set_arg('padded_vocab_size')
+    _set_arg('use_rmsnorm', force=True)
+    _set_arg('no_persist_layer_norm', force=True)
     if checkpoint_version < 3.0:
         _set_arg('tensor_model_parallel_size',
                  'model_parallel_size')
@@ -504,7 +509,7 @@ def load_checkpoint(model, optimizer, opt_param_scheduler, load_arg='load', stri
 
     model = unwrap_model(model)
 
-    state_dict, checkpoint_name, release = _load_base_checkpoint(load_dir, rank0=False)
+    state_dict, checkpoint_name, release = _load_base_checkpoint(load_dir, args.load_iteration, rank0=False)
 
     # Checkpoint not loaded.
     if state_dict is None:
diff --git a/megatron/global_vars.py b/megatron/global_vars.py
index 4e0118e..fe079c1 100644
--- a/megatron/global_vars.py
+++ b/megatron/global_vars.py
@@ -19,6 +19,8 @@ _GLOBAL_TENSORBOARD_WRITER = None
 _GLOBAL_ADLR_AUTORESUME = None
 _GLOBAL_TIMERS = None
 _GLOBAL_SIGNAL_HANDLER = None
+_GLOBAL_WANDB = None
+
 
 def get_args():
     """Return arguments."""
@@ -56,6 +58,12 @@ def get_tensorboard_writer():
     return _GLOBAL_TENSORBOARD_WRITER
 
 
+def get_wandb():
+    """Return wandb object. It can be None so no need
+    to check if it is initialized"""
+    return _GLOBAL_WANDB
+
+
 def get_adlr_autoresume():
     """ADLR autoresume object. It can be None so no need
     to check if it is initialized."""
@@ -92,12 +100,13 @@ def set_global_variables(args, build_tokenizer=True):
     if build_tokenizer:
         _ = _build_tokenizer(args)
     _set_tensorboard_writer(args)
+    _set_wandb(args)
     _set_adlr_autoresume(args)
     _set_timers(args)
 
     if args.exit_signal_handler:
         _set_signal_handler()
-    
+
 
 def set_args(args):
     global _GLOBAL_ARGS
@@ -153,6 +162,36 @@ def _set_tensorboard_writer(args):
                   'no TensorBoard logs will be written.', flush=True)
 
 
+def _set_wandb(args):
+    global _GLOBAL_WANDB
+    _ensure_var_is_not_initialized(_GLOBAL_WANDB,
+                                   'wandb writer')
+    is_local_main = (args.rank + 1) % torch.cuda.device_count() == 0
+    node_rank = args.rank // torch.cuda.device_count()
+    description = os.environ.get('RUN_DESCRIPTION', default='')
+    if hasattr(args, 'wandb_project') and \
+            args.wandb_project and is_local_main:
+        try:
+            import wandb
+            is_master = args.rank == (args.world_size - 1)
+            wandb.init(
+                project=args.wandb_project,
+                group=args.wandb_group,
+                name=args.wandb_master_name if is_master
+                else f'{args.wandb_worker_name}-{node_rank}',
+                save_code=False,
+                config=args,
+                force=False,
+                notes=description,
+                tags=['master'if is_master else 'worker']
+            )
+            if args.rank == (args.world_size - 1):
+                _GLOBAL_WANDB = wandb
+        except Exception:
+            print("WARNING: Skip wandb setup. Please execute "
+                  "'wandb login' to enable wandb.", flush=True)
+
+
 def _set_adlr_autoresume(args):
     """Initialize ADLR autoresume."""
     global _GLOBAL_ADLR_AUTORESUME
@@ -186,6 +225,3 @@ def _ensure_var_is_initialized(var, name):
 def _ensure_var_is_not_initialized(var, name):
     """Make sure the input variable is not None."""
     assert var is None, '{} is already initialized.'.format(name)
-
-
-
diff --git a/megatron/model/fused_layer_norm.py b/megatron/model/fused_layer_norm.py
index fd8591e..e57cba6 100644
--- a/megatron/model/fused_layer_norm.py
+++ b/megatron/model/fused_layer_norm.py
@@ -11,6 +11,7 @@ from torch.nn import init
 import importlib
 
 from megatron.core.utils import make_viewless_tensor
+from megatron import get_args
 
 try:
     from apex.contrib.layer_norm.layer_norm import FastLayerNormFN
@@ -18,7 +19,7 @@ try:
 except:
     HAVE_PERSIST_LAYER_NORM = False
 
-from apex.normalization.fused_layer_norm import FusedLayerNormAffineFunction
+from apex.normalization.fused_layer_norm import FusedLayerNormAffineFunction, FusedRMSNormAffineFunction
 
 
 global fused_layer_norm_cuda
@@ -32,6 +33,8 @@ class MixedFusedLayerNorm(torch.nn.Module):
                sequence_parallel=False,
                apply_layernorm_1p=False):
         super(MixedFusedLayerNorm, self).__init__()
+        args = get_args()
+        self.use_rmsnorm = args.use_rmsnorm
 
         self.apply_layernorm_1p = apply_layernorm_1p
 
@@ -77,7 +80,10 @@ class MixedFusedLayerNorm(torch.nn.Module):
     weight = self.weight + 1 if self.apply_layernorm_1p else self.weight
 
     if self.no_persist_layer_norm:
-        return FusedLayerNormAffineFunction.apply(input, weight, self.bias, self.normalized_shape, self.eps)
+        if not self.use_rmsnorm:
+            return FusedLayerNormAffineFunction.apply(input, weight, self.bias, self.normalized_shape, self.eps)
+        else:
+            return FusedRMSNormAffineFunction.apply(input, weight, self.normalized_shape, self.eps)
     else:
         output = FastLayerNormFN.apply(input, weight, self.bias, self.eps)
 
diff --git a/megatron/text_generation/api.py b/megatron/text_generation/api.py
index 090b630..726c82d 100644
--- a/megatron/text_generation/api.py
+++ b/megatron/text_generation/api.py
@@ -8,26 +8,28 @@ import torch
 from megatron.core import mpu
 from .communication import broadcast_float_list
 from .generation import (
-        generate_tokens_probs_and_return_on_first_stage,
-        score_and_return_on_first_stage,
-        beam_search_and_return_on_first_stage)
+    generate_tokens_probs_and_return_on_first_stage,
+    score_and_return_on_first_stage,
+    beam_search_and_return_on_first_stage)
 from .tokenization import (
     tokenize_prompts,
+    tokenize_sequences,
     detokenize_generations)
 
+
 def generate_and_post_process(model,
                               prompts=None,
                               tokens_to_generate=0,
                               return_output_log_probs=False,
+                              echo_prompts=False,
                               top_k_sampling=0,
                               top_p_sampling=0.0,
                               top_p_decay=0.0,
                               top_p_bound=0.0,
                               temperature=1.0,
                               add_BOS=False,
-                              use_eod_token_for_early_termination=True,
-                              stop_on_double_eol=False,
-                              stop_on_eol=False,
+                              use_stop_tokens_for_early_termination=True,
+                              stop_sequences=None,
                               prevent_newline_after_colon=False,
                               random_seed=-1):
     """Run inference and post-process outputs, i.e., detokenize,
@@ -38,6 +40,7 @@ def generate_and_post_process(model,
         model,
         prompts=prompts,
         tokens_to_generate=tokens_to_generate,
+        echo_prompts=echo_prompts,
         return_output_log_probs=return_output_log_probs,
         top_k_sampling=top_k_sampling,
         top_p_sampling=top_p_sampling,
@@ -45,45 +48,44 @@ def generate_and_post_process(model,
         top_p_bound=top_p_bound,
         temperature=temperature,
         add_BOS=add_BOS,
-        use_eod_token_for_early_termination=use_eod_token_for_early_termination,
-        stop_on_double_eol=stop_on_double_eol,
-        stop_on_eol=stop_on_eol,
+        use_stop_tokens_for_early_termination=use_stop_tokens_for_early_termination,
+        stop_sequences=stop_sequences,
         prevent_newline_after_colon=prevent_newline_after_colon,
         random_seed=random_seed)
 
     # Only post-process on first stage.
     if mpu.is_pipeline_first_stage():
-        tokens, prompts_plus_generations, prompts_plus_generations_segments = \
-            detokenize_generations(tokens, lengths, True)
-
         if return_output_log_probs:
+            tokens, generations, generations_segments = \
+                detokenize_generations(tokens, lengths, return_output_log_probs)
             output_log_probs = output_log_probs.cpu().numpy().tolist()
-            for i, (prob, seg) in enumerate(zip(output_log_probs, prompts_plus_generations_segments)):
+            for i, (prob, seg) in enumerate(zip(output_log_probs, generations_segments)):
                 output_log_probs[i] = prob[:len(seg)-1]
-
-        return prompts_plus_generations, prompts_plus_generations_segments, \
-            output_log_probs, tokens
-
+            return generations, generations_segments, \
+                output_log_probs, tokens
+        else:
+            tokens, generations = detokenize_generations(tokens, lengths, return_output_log_probs)
+            return generations, None, None, tokens
     return None
 
 def generate(model,
              prompts=None,
              tokens_to_generate=0,
              return_output_log_probs=False,
+             echo_prompts=False,
              top_k_sampling=0,
              top_p_sampling=0.0,
              top_p_decay=0.0,
              top_p_bound=0.0,
              temperature=1.0,
              add_BOS=False,
-             use_eod_token_for_early_termination=True,
-             stop_on_double_eol=False,
-             stop_on_eol=False,
+             use_stop_tokens_for_early_termination=True,
+             stop_sequences=None,
              prevent_newline_after_colon=False,
              random_seed=-1):
     """Given prompts and input parameters, run inference and return:
        tokens: prompts plus the generated tokens.
-       lengths: length of the prompt + generations. Note that we can
+       lengths: length of the (prompt +) generations. Note that we can
            discard tokens in the tokens tensor that are after the
            corresponding length.
        output_log_probs: log probs of the tokens.
@@ -93,11 +95,22 @@ def generate(model,
     values = [tokens_to_generate,
               return_output_log_probs,
               top_k_sampling, top_p_sampling, top_p_decay, top_p_bound,
-              temperature, add_BOS, use_eod_token_for_early_termination,
-              stop_on_double_eol,
-              stop_on_eol,
+              temperature, add_BOS, use_stop_tokens_for_early_termination,
               prevent_newline_after_colon,
               random_seed]
+    if stop_sequences != None:
+        stop_tokens = []
+        for i, tokens in enumerate(tokenize_sequences(stop_sequences)):
+            if len(tokens) == 1:
+                stop_tokens.append(tokens[0])
+            else:
+                print(
+                    f"Stop sequence [{stop_sequences[i]}] is not supported because its tokenized length exceeds 1")
+        stop_tokens = torch.tensor(stop_tokens, dtype=torch.int64)
+        values.append(len(stop_tokens))
+        values.extend(stop_tokens)
+    else:
+        values.append(0)
     values_float_tensor = broadcast_float_list(len(values), float_list=values)
     tokens_to_generate = int(values_float_tensor[0].item())
     return_output_log_probs = bool(values_float_tensor[1].item())
@@ -107,11 +120,14 @@ def generate(model,
     top_p_bound = values_float_tensor[5].item()
     temperature = values_float_tensor[6].item()
     add_BOS = bool(values_float_tensor[7].item())
-    use_eod_token_for_early_termination = bool(values_float_tensor[8].item())
-    stop_on_double_eol = bool(values_float_tensor[9].item())
-    stop_on_eol = bool(values_float_tensor[10].item())
-    prevent_newline_after_colon = bool(values_float_tensor[11].item())
-    random_seed = int(values_float_tensor[12].item())
+    use_stop_tokens_for_early_termination = bool(values_float_tensor[8].item())
+    prevent_newline_after_colon = bool(values_float_tensor[9].item())
+    random_seed = int(values_float_tensor[10].item())
+    stop_tokens_length = int(values_float_tensor[11].item())
+    if stop_tokens_length > 0:
+        stop_tokens = values_float_tensor[12: 12 + stop_tokens_length].int()
+    else:
+        stop_tokens = None
 
     if random_seed != -1:
         torch.random.manual_seed(random_seed)
@@ -120,14 +136,14 @@ def generate(model,
     # Note that these tensors are broadcaseted to all ranks.
     if torch.distributed.get_rank() == 0:
         assert prompts is not None
-    
+
     context_tokens_tensor, context_length_tensor = tokenize_prompts(
         prompts=prompts, tokens_to_generate=tokens_to_generate, add_BOS=add_BOS)
 
     if tokens_to_generate == 0:
         return score_and_return_on_first_stage(
             model, context_tokens_tensor, context_length_tensor)
-    
+
     # Main inference function.
     # Note that the outputs are available on the first stage.
     return generate_tokens_probs_and_return_on_first_stage(
@@ -138,10 +154,10 @@ def generate(model,
         top_p_decay=top_p_decay,
         top_p_bound=top_p_bound,
         temperature=temperature,
-        use_eod_token_for_early_termination=use_eod_token_for_early_termination,
-        stop_on_double_eol=stop_on_double_eol,
-        stop_on_eol=stop_on_eol,
-        prevent_newline_after_colon=prevent_newline_after_colon)
+        use_stop_tokens_for_early_termination=use_stop_tokens_for_early_termination,
+        stop_tokens=stop_tokens,
+        prevent_newline_after_colon=prevent_newline_after_colon,
+        echo_prompts=echo_prompts)
 
 def beam_search_and_post_process(model,
                                  prompts=None,
diff --git a/megatron/text_generation/generation.py b/megatron/text_generation/generation.py
index 098706e..4e73d0e 100644
--- a/megatron/text_generation/generation.py
+++ b/megatron/text_generation/generation.py
@@ -35,7 +35,7 @@ def score_and_return_on_first_stage(model, tokens, lengths):
     assert max_prompt_length == tokens.size(1)
     
     if max_prompt_length > args.max_position_embeddings:
-        raise ValueError("Length of prompt + tokens_to_generate longer than allowed")
+        raise ValueError(f"Length of prompt + tokens_to_generate ({max_prompt_length}) longer than allowed ({args.max_position_embeddings})")
     
     if max_prompt_length * batch_size > args.max_tokens_to_oom:
         raise ValueError("Too many tokens.  " + str(max_prompt_length*batch_size)+ " is greater than "+str(args.max_tokens_to_oom))
@@ -90,10 +90,10 @@ def generate_tokens_probs_and_return_on_first_stage(
         return_output_log_probs=False,
         top_k=0, top_p=0.0, top_p_decay=0.0, top_p_bound=0.0,
         temperature=1.0,
-        use_eod_token_for_early_termination=True,
-        stop_on_double_eol=False,
-        stop_on_eol=False,
-        prevent_newline_after_colon=True
+        use_stop_tokens_for_early_termination=True,
+        stop_tokens=None,
+        prevent_newline_after_colon=True,
+        echo_prompts=False
         ):
     """Main token generation function.
     Arguments:
@@ -109,8 +109,8 @@ def generate_tokens_probs_and_return_on_first_stage(
                 if top-k > 0 then we expect top-p=0.
                 if top-p > 0 then we check for top-k=0.
         temperature: sampling temperature.
-        use_eod_token_for_early_termination: if True, do early termination if
-            all the sequences have reached this token.
+        use_stop_tokens_for_early_termination: if True, do early termination if
+            all the sequences have reached stop tokens.
         prevent_newline_after_colon: if True, it will disable generating new line \n after :
     Note: Outside of model, other parameters only need to be available on
           rank 0.
@@ -130,7 +130,7 @@ def generate_tokens_probs_and_return_on_first_stage(
     max_sequence_length = tokens.size(1)
 
     if max_sequence_length > args.max_position_embeddings:
-        raise ValueError("Length of prompt + tokens_to_generate longer than allowed")
+        raise ValueError(f"Length of prompt + tokens_to_generate ({max_sequence_length}) longer than allowed ({args.max_position_embeddings})")
     
     if max_sequence_length * batch_size > args.max_tokens_to_oom:
         raise ValueError("Too many tokens.  " + str(max_sequence_length*batch_size)+ " is greater than "+str(args.max_tokens_to_oom))
@@ -213,19 +213,18 @@ def generate_tokens_probs_and_return_on_first_stage(
                 # Calculate the log probabilities.
                 if return_output_log_probs:
                     log_probs = F.log_softmax(logits, dim=2)
-                    if return_output_log_probs:
-                        # Pick the tokens that we need to get the log
-                        # probabilities for. Note that next input token is
-                        # the token which we selected in the current logits,
-                        # so shift by 1.
-                        indices = torch.unsqueeze(
-                            tokens[
-                                :,
-                                (prev_context_length + 1):(context_length + 1)],
-                            2)
-                        output_log_probs[:,
-                                         prev_context_length:context_length] = \
-                            torch.gather(log_probs, 2, indices).squeeze(2)
+                    # Pick the tokens that we need to get the log
+                    # probabilities for. Note that next input token is
+                    # the token which we selected in the current logits,
+                    # so shift by 1.
+                    indices = torch.unsqueeze(
+                        tokens[
+                            :,
+                            (prev_context_length + 1):(context_length + 1)],
+                        2)
+                    output_log_probs[:,
+                                        prev_context_length:context_length] = \
+                        torch.gather(log_probs, 2, indices).squeeze(2)
 
             # Update the tokens on the first stage so the next input to
             # the network is correct.
@@ -240,14 +239,11 @@ def generate_tokens_probs_and_return_on_first_stage(
             if mpu.is_pipeline_last_stage():
                 # TODO(rprenger) These stopping methods are tokenizer dependent
                 # instead tokenization should be in the inference loop so stop sequences can be used
-                if stop_on_double_eol:
-                    hit_double_eol = (new_sample == 628).byte() & started.byte()
-                    hit_two_eols = (new_sample == 198).byte() & (tokens[:, context_length-1] == 198).byte() & started.byte()
-                    done_token = hit_double_eol | hit_two_eols
-                elif stop_on_eol:
-                    hit_double_eol = (new_sample == 628).byte() & started.byte()
-                    hit_eol = (new_sample == 198).byte() & started.byte()
-                    done_token = hit_double_eol | hit_eol
+                if stop_tokens is not None and len(stop_tokens) > 0:
+                    done_token = torch.any(
+                        new_sample.expand(stop_tokens.shape[0], new_sample.shape[0]) == stop_tokens.unsqueeze(dim=1), dim=0) \
+                        & started.byte()
+
                 else: 
                     done_token = (new_sample == termination_id).byte() & \
                         started.byte()
@@ -259,7 +255,7 @@ def generate_tokens_probs_and_return_on_first_stage(
                 done = torch.all(is_generation_done)
             done = broadcast_from_last_pipeline_stage(1, torch.uint8,
                                                       tensor=done)
-            if use_eod_token_for_early_termination and done:
+            if use_stop_tokens_for_early_termination and done:
                 break
             
     # ===================================================
@@ -269,7 +265,7 @@ def generate_tokens_probs_and_return_on_first_stage(
     tokens = tokens[:, :(context_length + 1)]
     if mpu.is_pipeline_last_stage():
         if return_output_log_probs:
-            output_log_probs = output_log_probs[:, :context_length]
+            output_log_probs = output_log_probs[:, :context_length].contiguous()
 
     # ======================================
     # Broadcast to the first pipeline stage.
@@ -281,7 +277,13 @@ def generate_tokens_probs_and_return_on_first_stage(
         output_log_probs_size = (batch_size, context_length)
         output_log_probs = broadcast_from_last_to_first_pipeline_stage(
             output_log_probs_size, torch.float32, output_log_probs)
-
+    if not echo_prompts and mpu.is_pipeline_first_stage():
+        generated_sequence_lengths -= lengths
+        for i, (sequence, length) in enumerate(zip(tokens, lengths)):
+            tokens[i] = sequence.roll(-length.item(), dims=0)
+        if return_output_log_probs:
+            for i, (prob, length) in enumerate(zip(output_log_probs, lengths)):
+                output_log_probs[i] = prob.roll(-(length.item() - 1), dims=0)
     return tokens, generated_sequence_lengths, output_log_probs
 
 def beam_search_and_return_on_first_stage(model, tokens, lengths, beam_size, stop_token, num_return_gen, length_penalty, prevent_newline_after_colon=True):
diff --git a/megatron/text_generation/tokenization.py b/megatron/text_generation/tokenization.py
index accead3..3e8caa5 100644
--- a/megatron/text_generation/tokenization.py
+++ b/megatron/text_generation/tokenization.py
@@ -32,7 +32,7 @@ def detokenize_generations(tokens_gpu_tensor,
             for token in sequence_tokens:
                 if args.tokenizer_type in ['SentencePieceTokenizer', 
                         'GPTSentencePieceTokenizer']:
-                    word = tokenizer.decoder[token]
+                    word = tokenizer.detokenize([token])
                 elif args.tokenizer_type == 'NullTokenizer':
                     word = str(token)
                 else:
@@ -49,6 +49,12 @@ def detokenize_generations(tokens_gpu_tensor,
 
     return tokens, prompts_plus_generations
 
+def tokenize_sequences(sequences=None):
+    sequence_tokens = None
+    if sequences != None:
+        tokenizer = get_tokenizer()
+        sequence_tokens = [tokenizer.tokenize(ele) for ele in sequences]
+    return sequence_tokens
 
 def tokenize_prompts(prompts=None, tokens_to_generate=None,
                      add_BOS=None, rank=0):
diff --git a/megatron/text_generation_server.py b/megatron/text_generation_server.py
index 58550f2..60f8fc8 100644
--- a/megatron/text_generation_server.py
+++ b/megatron/text_generation_server.py
@@ -1,13 +1,13 @@
 # Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
 import datetime
+import time
 import torch
 import json
 import threading
-from flask import Flask, request, jsonify, current_app
+import asyncio
+from flask import Flask, request, jsonify
 from flask_restful import Resource, Api
-from megatron import get_args
 from megatron.text_generation import generate_and_post_process
-from megatron.text_generation import beam_search_and_post_process
 
 
 GENERATE_NUM = 0
@@ -17,6 +17,8 @@ lock = threading.Lock()
 class MegatronGenerate(Resource):
     def __init__(self, model):
         self.model = model
+        asyncio.set_event_loop(asyncio.new_event_loop())
+        self.loop = asyncio.get_event_loop()
 
     @staticmethod
     def send_do_generate():
@@ -27,209 +29,216 @@ class MegatronGenerate(Resource):
     def send_do_beam_search():
         choice = torch.cuda.LongTensor([BEAM_NUM])
         torch.distributed.broadcast(choice, 0)
-    
+
+    def check(self, raw_req):
+        if not 'prompts' in raw_req:
+            return 'prompts argument required', 400
+        if len(raw_req['prompts']) == 0:
+            return "prompts is empty", 400
+        if len(raw_req['prompts']) > 128:
+            return "Maximum number of prompts is 128", 400
+
+    async def generate(self, req):
+        MegatronGenerate.send_do_generate()  # Tell other ranks we're doing generate
+        start_time = time.time()
+        response, response_seg, response_logprobs, _ = \
+            generate_and_post_process(
+            self.model,
+            prompts=req['prompts'],
+            tokens_to_generate=req['tokens_to_generate'],
+            echo_prompts=req['echo_prompts'],
+            return_output_log_probs=req['logprobs'],
+            top_k_sampling=req['top_k'],
+            top_p_sampling=req['top_p'],
+            top_p_decay=req['top_p_decay'],
+            top_p_bound=req['top_p_bound'],
+            temperature=req['temperature'],
+            add_BOS=req['add_BOS'],
+            use_stop_tokens_for_early_termination=True,
+            stop_sequences=req['stop_sequences'],
+            prevent_newline_after_colon=req['prevent_newline_after_colon'],
+            random_seed=req['random_seed'])
+        end_time = time.time()
+        print(f"Response(use {end_time - start_time}s): " + str(response))
+        return {
+            "text": response,
+            "segments": response_seg,
+            "logprobs": response_logprobs
+            }
+
     def put(self):
-        args = get_args()
-       
-        if not "prompts" in request.get_json():
+        raw_req = request.get_json()
+
+        if not "prompts" in raw_req:
             return "prompts argument required", 400
         
-        if "max_len" in request.get_json():
+        if "max_len" in raw_req:
             return "max_len is no longer used.  Replace with tokens_to_generate", 400
         
-        if "sentences" in request.get_json():
+        if "sentences" in raw_req:
             return "sentences is no longer used.  Replace with prompts", 400
 
-        prompts = request.get_json()["prompts"]
-        if not isinstance(prompts, list):
+        if isinstance(raw_req["prompts"], str):
+            raw_req['prompts'] = [raw_req['prompts']]
+
+        if not isinstance(raw_req["prompts"], list):
             return "prompts is not a list of strings", 400
 
-        if len(prompts) == 0:
+        if len(raw_req['prompts']) == 0:
             return "prompts is empty", 400
         
-        if len(prompts) > 128:
+        if len(raw_req['prompts']) > 128:
             return "Maximum number of prompts is 128", 400
         
-        tokens_to_generate = 64  # Choosing hopefully sane default.  Full sequence is slow
-        if "tokens_to_generate" in request.get_json():
-            tokens_to_generate = request.get_json()["tokens_to_generate"]
-            if not isinstance(tokens_to_generate, int):
+        if 'tokens_to_generate' in raw_req:
+            if not isinstance(raw_req['tokens_to_generate'], int):
                 return "tokens_to_generate must be an integer greater than 0"
-            if tokens_to_generate < 0:
+            if raw_req['tokens_to_generate'] < 0:
                 return "tokens_to_generate must be an integer greater than or equal to 0"
+        else:
+            raw_req['tokens_to_generate'] = 64
 
         logprobs = False
-        if "logprobs" in request.get_json():
-            logprobs = request.get_json()["logprobs"]
+        if "logprobs" in raw_req:
+            logprobs = raw_req["logprobs"]
             if not isinstance(logprobs, bool):
                 return "logprobs must be a boolean value"
-        
-        if tokens_to_generate == 0 and not logprobs:
-            return "tokens_to_generate=0 implies logprobs should be True"
-        
-        temperature = 1.0
-        if "temperature" in request.get_json():
-            temperature = request.get_json()["temperature"]
-            if not (type(temperature) == int or type(temperature) == float):
-                return "temperature must be a positive number less than or equal to 100.0"
-            if not (0.0 < temperature <= 100.0):
-                return "temperature must be a positive number less than or equal to 100.0"
-        
+        else:
+            raw_req['logprobs'] = False
+
+        if raw_req['tokens_to_generate'] == 0 and not raw_req['logprobs']:
+            print("tokens_to_generate=0 implies logprobs should be True")
+            raw_req['logprobs'] = True
+        
+        if "echo_prompts" in raw_req:
+            if not isinstance(raw_req['echo_prompts'], bool):
+                return "echo_prompts must be a bool"
+        else:
+            raw_req['echo_prompts'] = False
+
         top_k = 0.0
-        if "top_k" in request.get_json():
-            top_k = request.get_json()["top_k"]
+        if "top_k" in raw_req:
+            top_k = raw_req["top_k"]
             if not (type(top_k) == int):
                 return "top_k must be an integer equal to or greater than 0 and less than or equal to 1000"
             if not (0 <= top_k <= 1000):
                 return "top_k must be equal to or greater than 0 and less than or equal to 1000"
+        else:
+            raw_req['top_k'] = 0.0
         
-        top_p = 0.0
-        if "top_p" in request.get_json():
-            top_p = request.get_json()["top_p"]
-            if not (type(top_p) == float):
+        if "top_p" in raw_req:
+            top_p = raw_req["top_p"]
+            if not (type(top_p) == float or type(top_p) == int):
                 return "top_p must be a positive float less than or equal to 1.0"
             if top_p > 0.0 and top_k > 0.0:
                 return "cannot set both top-k and top-p samplings."
             if not (0 <= top_p <= 1.0):
                 return "top_p must be less than or equal to 1.0"
+        else:
+            raw_req['top_p'] = 0.0
         
-        top_p_decay = 0.0
-        if "top_p_decay" in request.get_json():
-            top_p_decay = request.get_json()["top_p_decay"]
+        if "top_p_decay" in raw_req:
+            top_p_decay = raw_req["top_p_decay"]
             if not (type(top_p_decay) == float):
                 return "top_p_decay must be a positive float less than or equal to 1.0"
             if top_p == 0.0:
                 return "top_p_decay cannot be set without top_p"
             if not (0 <= top_p_decay <= 1.0):
                 return "top_p_decay must be less than or equal to 1.0"
-        
+        else:
+            raw_req['top_p_decay'] = 0.0
+
         top_p_bound = 0.0
-        if "top_p_bound" in request.get_json():
-            top_p_bound = request.get_json()["top_p_bound"]
+        if "top_p_bound" in raw_req:
+            top_p_bound = raw_req["top_p_bound"]
             if not (type(top_p_bound) == float):
                 return "top_p_bound must be a positive float less than or equal to top_p"
             if top_p == 0.0:
                 return "top_p_bound cannot be set without top_p"
             if not (0.0 < top_p_bound <= top_p):
                 return "top_p_bound must be greater than 0 and less than top_p"
-        
-        add_BOS = False
-        if "add_BOS" in request.get_json():
-            add_BOS = request.get_json()["add_BOS"]
-            if not isinstance(add_BOS, bool):
+        else:
+            raw_req['top_p_bound'] = 0.0
+
+        if "temperature" in raw_req:
+            temperature = raw_req["temperature"]
+            if not (type(temperature) == int or type(temperature) == float):
+                return "temperature must be a positive number less than or equal to 100.0"
+            if not (0.0 <= temperature <= 100.0):
+                return "temperature must be a positive number less than or equal to 100.0"
+        else:
+            raw_req['temperature'] = 0.0
+
+        if raw_req['temperature'] == 0.0:
+            raw_req['top_k'] = 1
+            raw_req['top_p'] = 0
+
+        if "add_BOS" in raw_req:
+            if not isinstance(raw_req["add_BOS"], bool):
                 return "add_BOS must be a boolean value"
+        else:
+            raw_req['add_BOS'] = False
         
-        if any([len(prompt) == 0 for prompt in prompts]) and not add_BOS:
+        if any([len(prompt) == 0 for prompt in raw_req['prompts']]) and not raw_req["add_BOS"]:
             return "Empty prompts require add_BOS=true"
 
-        stop_on_double_eol = False
-        if "stop_on_double_eol" in request.get_json():
-            stop_on_double_eol = request.get_json()["stop_on_double_eol"]
-            if not isinstance(stop_on_double_eol, bool):
-                return "stop_on_double_eol must be a boolean value"
-        
-        stop_on_eol = False
-        if "stop_on_eol" in request.get_json():
-            stop_on_eol = request.get_json()["stop_on_eol"]
-            if not isinstance(stop_on_eol, bool):
-                return "stop_on_eol must be a boolean value"
-
-        prevent_newline_after_colon = False
-        if "prevent_newline_after_colon" in request.get_json():
-            prevent_newline_after_colon = request.get_json()["prevent_newline_after_colon"]
-            if not isinstance(prevent_newline_after_colon, bool):
+        if "stop_sequences" in raw_req:
+            if not isinstance(raw_req["stop_sequences"], list):
+                return "stop_sequences must be a str list"
+            for seq in raw_req['stop_sequences']:
+                if not isinstance(seq, str):
+                    return "stop_sequences must be a str list"
+        else:
+            raw_req["stop_sequences"] = None
+
+        if "prevent_newline_after_colon" in raw_req:
+            if not isinstance(raw_req["prevent_newline_after_colon"], bool):
                 return "prevent_newline_after_colon must be a boolean value"
+        else:
+            raw_req['prevent_newline_after_colon'] = False
 
-        random_seed = -1
-        if "random_seed" in request.get_json():
-            random_seed = request.get_json()["random_seed"]
+        if "random_seed" in raw_req:
+            random_seed = raw_req["random_seed"]
             if not isinstance(random_seed, int):
                 return "random_seed must be integer"
             if random_seed < 0: 
                 return "random_seed must be a positive integer"
+        else:
+            raw_req['random_seed'] = 1234
 
         no_log = False
-        if "no_log" in request.get_json():
-            no_log = request.get_json()["no_log"]
+        if "no_log" in raw_req:
+            no_log = raw_req["no_log"]
             if not isinstance(no_log, bool):
                 return "no_log must be a boolean value"
         
         beam_width = None
-        if "beam_width" in request.get_json():
-            beam_width = request.get_json()["beam_width"]
+        if "beam_width" in raw_req:
+            beam_width = raw_req["beam_width"]
             if not isinstance(beam_width, int):
                 return "beam_width must be integer"
             if beam_width < 1:
                 return "beam_width must be an integer > 1"
-            if len(prompts) > 1:
+            if len(raw_req['prompts']) > 1:
                 return "When doing beam_search, batch size must be 1"
 
-        stop_token=50256
-        if "stop_token" in request.get_json():
-            stop_token = request.get_json()["stop_token"]
-            if not isinstance(stop_token, int):
-                return "stop_token must be an integer"
-        
-        length_penalty = 1 
-        if "length_penalty" in request.get_json():
-            length_penalty = request.get_json()["length_penalty"]
+        if "length_penalty" in raw_req:
+            length_penalty = raw_req["length_penalty"]
             if not isinstance(length_penalty, float):
                 return "length_penalty must be a float"
-        
-        with lock:  # Need to get lock to keep multiple threads from hitting code
-            
-            if not no_log:
-                print("request IP: " + str(request.remote_addr))
-                print(json.dumps(request.get_json()),flush=True)
-                print("start time: ", datetime.datetime.now())
-            
-            try:
-                if beam_width is not None:
-                    MegatronGenerate.send_do_beam_search()  # Tell other ranks we're doing beam_search
-                    response, response_seg, response_scores = \
-                        beam_search_and_post_process(
-                        self.model,
-                        prompts=prompts,
-                        tokens_to_generate=tokens_to_generate,
-                        beam_size = beam_width,
-                        add_BOS=add_BOS,
-                        stop_token=stop_token,
-                        num_return_gen=beam_width,  # Returning whole beam
-                        length_penalty=length_penalty,
-                        prevent_newline_after_colon=prevent_newline_after_colon
-                        )
-                    
-                    return jsonify({"text": response,
-                        "segments": response_seg,
-                        "scores": response_scores})
-                else:
-                    MegatronGenerate.send_do_generate()  # Tell other ranks we're doing generate
-                    response, response_seg, response_logprobs, _ = \
-                        generate_and_post_process(
-                        self.model,
-                        prompts=prompts,
-                        tokens_to_generate=tokens_to_generate,
-                        return_output_log_probs=logprobs,
-                        top_k_sampling=top_k,
-                        top_p_sampling=top_p,
-                        top_p_decay=top_p_decay,
-                        top_p_bound=top_p_bound,
-                        temperature=temperature,
-                        add_BOS=add_BOS,
-                        use_eod_token_for_early_termination=True,
-                        stop_on_double_eol=stop_on_double_eol,
-                        stop_on_eol=stop_on_eol,
-                        prevent_newline_after_colon=prevent_newline_after_colon,
-                        random_seed=random_seed)
-
-                    return jsonify({"text": response,
-                        "segments": response_seg,
-                        "logprobs": response_logprobs})
-
-            except ValueError as ve:
-                return ve.args[0]
-            print("end time: ", datetime.datetime.now())
-        
+        else:
+            raw_req['length_penalty'] = 1
+
+        if not no_log:
+            print("request IP: " + str(request.remote_addr))
+            print(json.dumps(raw_req),flush=True)
+            print("start time: ", datetime.datetime.now())
+        try:
+            result = self.loop.run_until_complete(self.generate(raw_req))
+            return jsonify(result)
+        except ValueError as ve:
+            return ve.args[0]
+
 
 class MegatronServer(object):
     def __init__(self, model):
@@ -237,5 +246,5 @@ class MegatronServer(object):
         api = Api(self.app)
         api.add_resource(MegatronGenerate, '/api', resource_class_args=[model])
         
-    def run(self, url): 
-        self.app.run(url, threaded=True, debug=False)
+    def run(self, host, port): 
+        self.app.run(host=host, port=port, threaded=True, debug=False)
diff --git a/megatron/timers.py b/megatron/timers.py
index a9478fa..8399ad9 100644
--- a/megatron/timers.py
+++ b/megatron/timers.py
@@ -287,7 +287,7 @@ class Timers:
             print(output_string, flush=True)
 
 
-    def write(self, names, writer, iteration, normalizer=1.0,
+    def write(self, names, writer, wandb, iteration, normalizer=1.0,
               reset=False, barrier=False):
         """Write timers to a tensorboard writer
         Note that we only report maximum time across ranks to tensorboard.
@@ -302,3 +302,9 @@ class Timers:
             for name in name_to_min_max_time:
                 _, max_time = name_to_min_max_time[name]
                 writer.add_scalar(name + '-time', max_time, iteration)
+        if wandb is not None:
+            wandb_log_dic = {}
+            for name in name_to_min_max_time:
+                _, max_time = name_to_min_max_time[name]
+                wandb_log_dic[f'timer/{name}'] = max_time
+            wandb.log(wandb_log_dic, iteration)
diff --git a/megatron/training.py b/megatron/training.py
index b821ae7..e0dfc54 100644
--- a/megatron/training.py
+++ b/megatron/training.py
@@ -15,6 +15,7 @@ from megatron import get_args
 from megatron import get_signal_handler
 from megatron import get_timers
 from megatron import get_tensorboard_writer
+from megatron import get_wandb
 from megatron import get_current_global_batch_size
 from megatron import get_num_microbatches
 from megatron import is_last_rank
@@ -171,14 +172,14 @@ def pretrain(train_valid_test_dataset_provider,
         evaluate_and_print_results(prefix, forward_step_func,
                                    valid_data_iterator, model,
                                    iteration, process_non_loss_data_func, config,
-                                   verbose=True, write_to_tensorboard=not args.skip_train)
+                                   verbose=True, write_to_tracker=not args.skip_train)
 
     if args.do_test:
         prefix = f'iteration {iteration} on test set'
         evaluate_and_print_results(prefix, forward_step_func,
                                    test_data_iterator, model,
                                    iteration, process_non_loss_data_func, config,
-                                   verbose=True, write_to_tensorboard=not args.skip_train)
+                                   verbose=True, write_to_tracker=not args.skip_train)
 
 
 def update_train_iters(args):
@@ -504,6 +505,7 @@ def training_log(loss_dict, total_loss_dict, learning_rate, iteration,
     args = get_args()
     timers = get_timers()
     writer = get_tensorboard_writer()
+    wandb = get_wandb()
 
     # Advanced, skipped, and Nan iterations.
     advanced_iters_key = 'advanced iterations'
@@ -571,68 +573,97 @@ def training_log(loss_dict, total_loss_dict, learning_rate, iteration,
 
     # Tensorboard values.
     # Timer requires all the ranks to call.
-    if args.log_timers_to_tensorboard and \
-       (iteration % args.tensorboard_log_interval == 0):
-        timers.write(timers_to_log, writer, iteration,
+    if args.log_timers_to_tracker and \
+       (iteration % args.tracker_log_interval == 0):
+        timers.write(timers_to_log, writer, wandb, iteration,
                      normalizer=total_iterations)
-    if writer and (iteration % args.tensorboard_log_interval == 0):
-        if args.log_learning_rate_to_tensorboard:
-            writer.add_scalar('learning-rate', learning_rate, iteration)
-            writer.add_scalar('learning-rate vs samples', learning_rate,
-                              args.consumed_train_samples)
-        if args.log_batch_size_to_tensorboard:
-            writer.add_scalar('batch-size', batch_size, iteration)
-            writer.add_scalar('batch-size vs samples', batch_size,
-                              args.consumed_train_samples)
-        for key in loss_dict:
-            writer.add_scalar(key , loss_dict[key], iteration)
-            writer.add_scalar(key + ' vs samples', loss_dict[key],
-                              args.consumed_train_samples)
-        if args.log_loss_scale_to_tensorboard:
-            writer.add_scalar('loss-scale', loss_scale, iteration)
-            writer.add_scalar('loss-scale vs samples', loss_scale,
-                              args.consumed_train_samples)
-        if args.log_world_size_to_tensorboard:
-            writer.add_scalar('world-size', args.world_size, iteration)
-            writer.add_scalar('world-size vs samples', args.world_size,
-                              args.consumed_train_samples)
-        if grad_norm is not None:
-            writer.add_scalar('grad-norm', grad_norm, iteration)
-            writer.add_scalar('grad-norm vs samples', grad_norm,
-                              args.consumed_train_samples)
-        if num_zeros_in_grad is not None:
-            writer.add_scalar('num-zeros', num_zeros_in_grad, iteration)
-            writer.add_scalar('num-zeros vs samples', num_zeros_in_grad,
-                              args.consumed_train_samples)
-        if params_norm is not None:
-            writer.add_scalar('params-norm', params_norm, iteration)
-            writer.add_scalar('params-norm vs samples', params_norm,
-                              args.consumed_train_samples)
-        if args.log_memory_to_tensorboard:
-            mem_stats = torch.cuda.memory_stats()
-            writer.add_scalar(
-                "mem-reserved-bytes",
-                mem_stats["reserved_bytes.all.current"],
-                iteration,
-            )
-            writer.add_scalar(
-                "mem-allocated-bytes",
-                mem_stats["allocated_bytes.all.current"],
-                iteration,
-            )
-            writer.add_scalar(
-                "mem-allocated-count",
-                mem_stats["allocation.all.current"],
-                iteration,
-            )
+    if iteration % args.tracker_log_interval == 0:
+        if writer:
+            if args.log_learning_rate_to_tracker:
+                writer.add_scalar('learning-rate', learning_rate, iteration)
+                writer.add_scalar('learning-rate vs samples', learning_rate,
+                                args.consumed_train_samples)
+            if args.log_batch_size_to_tracker:
+                writer.add_scalar('batch-size', batch_size, iteration)
+                writer.add_scalar('batch-size vs samples', batch_size,
+                                args.consumed_train_samples)
+            for key in loss_dict:
+                writer.add_scalar(key , loss_dict[key], iteration)
+                writer.add_scalar(key + ' vs samples', loss_dict[key],
+                                args.consumed_train_samples)
+            if args.log_loss_scale_to_tracker:
+                writer.add_scalar('loss-scale', loss_scale, iteration)
+                writer.add_scalar('loss-scale vs samples', loss_scale,
+                                args.consumed_train_samples)
+            if args.log_world_size_to_tracker:
+                writer.add_scalar('world-size', args.world_size, iteration)
+                writer.add_scalar('world-size vs samples', args.world_size,
+                                args.consumed_train_samples)
+            if grad_norm is not None:
+                writer.add_scalar('grad-norm', grad_norm, iteration)
+                writer.add_scalar('grad-norm vs samples', grad_norm,
+                                args.consumed_train_samples)
+            if num_zeros_in_grad is not None:
+                writer.add_scalar('num-zeros', num_zeros_in_grad, iteration)
+                writer.add_scalar('num-zeros vs samples', num_zeros_in_grad,
+                                args.consumed_train_samples)
+            if params_norm is not None:
+                writer.add_scalar('params-norm', params_norm, iteration)
+                writer.add_scalar('params-norm vs samples', params_norm,
+                                args.consumed_train_samples)
+            if args.log_memory_to_tracker:
+                writer.add_scalar(
+                    "mem-reserved-bytes",
+                    mem_stats["reserved_bytes.all.current"],
+                    iteration,
+                )
+                writer.add_scalar(
+                    "mem-allocated-bytes",
+                    mem_stats["allocated_bytes.all.current"],
+                    iteration,
+                )
+                writer.add_scalar(
+                    "mem-allocated-count",
+                    mem_stats["allocation.all.current"],
+                    iteration,
+                )
+        if wandb:
+            wandb_log_dic = {}
+            if args.log_learning_rate_to_tracker:
+                wandb_log_dic['train/learning_rate'] = learning_rate
+            if args.log_batch_size_to_tracker:
+                wandb_log_dic['train/global_batch_size'] = batch_size
+            for key in loss_dict:
+                wandb_log_dic[f'train/{key}'] = loss_dict[key]
+            if args.log_loss_scale_to_tracker:
+                wandb_log_dic['train/loss_scale'] = loss_scale
+            if args.log_world_size_to_tracker:
+                wandb_log_dic['train/world_size'] = args.world_size
+            if grad_norm is not None:
+                wandb_log_dic['train/grad_norm'] = grad_norm
+            if num_zeros_in_grad is not None:
+                wandb_log_dic['train/num_zeros_in_grad'] = num_zeros_in_grad
+            if params_norm is not None:
+                wandb_log_dic['train/params_norm'] = params_norm
+            if args.log_memory_to_tracker:
+                mem_stats = torch.cuda.memory_stats()
+                wandb_log_dic['train/mem-reserved-bytes'] = mem_stats["reserved_bytes.all.current"]
+                wandb_log_dic['train/mem-allocated-bytes'] = mem_stats["allocated_bytes.all.current"]
+                wandb_log_dic['train/mem-allocated-count'] = mem_stats["allocation.all.current"]
+            wandb.log(wandb_log_dic, iteration)
 
     if iteration % args.log_interval == 0:
         elapsed_time = timers('interval-time').elapsed(barrier=True)
         elapsed_time_per_iteration = elapsed_time / total_iterations
-        if writer:
-            if args.log_timers_to_tensorboard:
+        samples_per_second = batch_size / elapsed_time_per_iteration
+        tokens_per_second = samples_per_second * args.seq_length
+        wandb_log_dic = {}
+        if args.log_timers_to_tracker:
+            if writer:
                 writer.add_scalar('iteration-time',
                                   elapsed_time_per_iteration, iteration)
+            if wandb:
+                wandb_log_dic['timer/time_per_iteration'] = elapsed_time_per_iteration
         log_string = ' iteration {:8d}/{:8d} |'.format(
             iteration, args.train_iters)
         log_string += ' consumed samples: {:12d} |'.format(
@@ -641,6 +672,8 @@ def training_log(loss_dict, total_loss_dict, learning_rate, iteration,
             elapsed_time_per_iteration * 1000.0)
         log_string += ' learning rate: {:.3E} |'.format(learning_rate)
         log_string += ' global batch size: {:5d} |'.format(batch_size)
+        log_string += ' samples per second: {:.3f} |'.format(samples_per_second)
+        log_string += ' tokens per second: {:.3f} |'.format(tokens_per_second)
         for key in total_loss_dict:
             if key not in [advanced_iters_key, skipped_iters_key,
                            nan_iters_key]:
@@ -651,7 +684,11 @@ def training_log(loss_dict, total_loss_dict, learning_rate, iteration,
                 total_loss_dict[key] = torch.cuda.FloatTensor([0.0])
         log_string += ' loss scale: {:.1f} |'.format(loss_scale)
         if grad_norm is not None:
-            log_string += ' grad norm: {:.3f} |'.format(grad_norm)
+            if isinstance(grad_norm, dict):
+                log_string += ' total grad norm: {:.3f} |'.format(grad_norm['total_grad_norm'])
+                log_string += ' embedding grad norm: {:.3f} |'.format(grad_norm['embed_grad_norm'])
+            else:
+                log_string += ' grad norm: {:.3f} |'.format(grad_norm)
         if num_zeros_in_grad is not None:
             log_string += ' num zeros: {:.1f} |'.format(num_zeros_in_grad)
         if params_norm is not None:
@@ -660,6 +697,17 @@ def training_log(loss_dict, total_loss_dict, learning_rate, iteration,
             total_loss_dict[skipped_iters_key])
         log_string += ' number of nan iterations: {:3d} |'.format(
             total_loss_dict[nan_iters_key])
+        if writer:
+            writer.add_scalar('samples_per_second', samples_per_second, iteration)
+            writer.add_scalar('tokens_per_second', tokens_per_second, iteration)
+            writer.add_scalar('skipped_iterations', total_loss_dict[skipped_iters_key], iteration)
+            writer.add_scalar('nan_iterations', total_loss_dict[nan_iters_key], iteration)
+        if wandb:
+            wandb_log_dic['train/samples_per_second'] = samples_per_second
+            wandb_log_dic['train/tokens_per_second'] = samples_per_second * args.seq_length
+            wandb_log_dic['train/skipped_iterations'] = total_loss_dict[skipped_iters_key]
+            wandb_log_dic['train/nan_iterations'] = total_loss_dict[nan_iters_key]
+            wandb.log(wandb_log_dic, iteration)
         total_loss_dict[advanced_iters_key] = 0
         total_loss_dict[skipped_iters_key] = 0
         total_loss_dict[nan_iters_key] = 0
@@ -889,10 +937,10 @@ def evaluate(forward_step_func,
 def evaluate_and_print_results(prefix, forward_step_func,
                                data_iterator, model,
                                iteration, process_non_loss_data_func, config,
-                               verbose=False, write_to_tensorboard=True):
+                               verbose=False, write_to_tracker=True):
     """Helper function to evaluate and dump results on screen."""
     args = get_args()
-    if write_to_tensorboard:
+    if write_to_tracker:
         writer = get_tensorboard_writer()
     else:
         writer = None
@@ -912,7 +960,7 @@ def evaluate_and_print_results(prefix, forward_step_func,
             writer.add_scalar('{} validation vs samples'.format(key),
                               total_loss_dict[key].item(),
                               args.consumed_train_samples)
-            if args.log_validation_ppl_to_tensorboard:
+            if args.log_validation_ppl_to_tracker:
                 writer.add_scalar('{} validation ppl'.format(key), ppl,
                                   iteration)
                 writer.add_scalar('{} validation ppl vs samples'.format(key),
diff --git a/tools/inference.py b/tools/inference.py
new file mode 100644
index 0000000..2d83fc6
--- /dev/null
+++ b/tools/inference.py
@@ -0,0 +1,207 @@
+
+"""Inference tools."""
+from abc import ABC, abstractmethod
+import time
+import jsonlines
+import torch
+import os
+import sys
+sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__),
+                                             os.path.pardir)))
+from megatron import get_args, print_rank_0
+from megatron.core import mpu
+from megatron.checkpointing import load_checkpoint
+from megatron.model import GPTModel
+from megatron.initialize import initialize_megatron
+from megatron.training import get_model
+from megatron.text_generation import generate_and_post_process
+
+
+class JsonlineReader():
+    def __init__(self, input):
+        self.input = jsonlines.open(input, 'r')
+
+    def __iter__(self):
+        return self
+
+    def __next__(self):
+        try:
+            line = self.input.read(skip_empty=True, allow_none=False)
+        except EOFError:
+            raise StopIteration
+        return line
+
+
+class JsonlineWriter():
+    def __init__(self, output):
+        self.output = jsonlines.open(output, 'w', flush=True)
+
+    def write(self, response):
+        self.output.write(response)
+
+
+class AbstractFormatter(ABC):
+
+    @abstractmethod
+    def format_input(self, request):
+        raise NotImplementedError(
+            f'FORMAT_INPUT is not provided for {self.name}')
+
+    @abstractmethod
+    def format_output(self, request, response):
+        raise NotImplementedError(
+            f'FORMAT_OUTPUT is not provided for {self.name}')
+
+
+class Formatter(AbstractFormatter):
+    """Default formatter implementation"""
+
+    def format_input(self, request):
+        if not isinstance(request['text'], list):
+            request['text'] = [request['text']]
+        return request
+
+    def format_output(self, request, response):
+        return {
+            'prompt': request['text'],
+            'text': response['text'],
+            'segments': response['segments'],
+            'logprobs': response['logprobs']
+        }
+
+
+class GPTEvalFormatter(Formatter):
+    """Formatter for FastChat llm-judge"""
+
+    def format_input(self, request):
+        if not isinstance(request['text'], list):
+            request['text'] = [request['text']]
+        request['text'] = [f"Question:{text}\\n\\nAnswer:" for text in request['text']]
+        return request
+
+    def format_output(self, request, response):
+        return {
+            'question_id': request['question_id'],
+            'text': response['text'][0],
+            'model_id': response['model_name'],
+            'metadata': {}
+        }
+
+
+def load_formatter(name):
+    if name == None:
+        return Formatter()
+    elif name == 'gpt_eval':
+        return GPTEvalFormatter()
+    else:
+        raise NotImplementedError(f"Formatter for {name} is not implemented")
+
+
+RUN_SIG = 0
+STOP_SIG = 1
+
+def run_infer(model, prompts, writer, formatter):
+    start_time = time.time()
+    state = torch.cuda.LongTensor([RUN_SIG])
+    torch.distributed.broadcast(state, 0)
+    request = [prompt['text'][0] for prompt in prompts]
+    print(f'request: {request}')
+    texts, segments, logprobs, _ = generate_and_post_process(
+        model,
+        prompts=request,
+        tokens_to_generate=args.tokens_to_generate,
+        echo_prompts=args.echo_prompts,
+        return_output_log_probs=args.log_probs,
+        top_k_sampling=args.top_k,
+        top_p_sampling=args.top_p,
+        temperature=args.temperature
+    )
+    end_time = time.time()
+    print(f'response: {texts}')
+    print(f'inference time: {end_time - start_time}')
+    for i, prompt in enumerate(prompts):
+        result = formatter.format_output(
+            request=prompt,
+            response={
+                'text': [texts[i]],
+                'segments': [segments[i]] if segments is not None else None,
+                'logprobs': [logprobs[i]] if logprobs is not None else None,
+                'model_name': args.model_name
+            })
+        writer.write(result)
+
+def infer(model, args):
+    if mpu.is_pipeline_first_stage() and mpu.get_tensor_model_parallel_rank() == 0:
+        reader = JsonlineReader(args.input)
+        writer = JsonlineWriter(args.output)
+        formatter = load_formatter(args.formatter)
+        prompts = []
+        for prompt in reader:
+            prompts.append(formatter.format_input(prompt))
+            if len(prompts) >= args.batch:
+                run_infer(model, prompts, writer, formatter)
+                prompts.clear()
+        if len(prompts) > 0:
+            run_infer(model, prompts, writer, formatter)
+        state = torch.cuda.LongTensor([STOP_SIG])
+        torch.distributed.broadcast(state, 0)
+    else:
+        while True:
+            state = torch.cuda.LongTensor(1)
+            torch.distributed.broadcast(state, 0)
+            if state[0].item() == RUN_SIG:
+                generate_and_post_process(model)
+            else:
+                break
+    print(f"rank {torch.distributed.get_rank()} finish inference")
+
+
+def model_provider(pre_process=True, post_process=True):
+    """Build the model."""
+
+    print_rank_0('building GPT model ...')
+    model = GPTModel(num_tokentypes=0, parallel_output=False,
+                     pre_process=pre_process, post_process=post_process)
+
+    return model
+
+
+def add_inference_args(parser):
+    group = parser.add_argument_group(title='inference')
+    group.add_argument('--input', type=str, required=True)
+    group.add_argument('--output', type=str, required=True)
+    group.add_argument('--formatter', type=str, default=None)
+    group.add_argument('--tokens-to-generate', type=int, default=512)
+    group.add_argument('--top-k', type=int, default=0)
+    group.add_argument('--top-p', type=float, default=0)
+    group.add_argument('--temperature', type=float, default=1.0)
+    group.add_argument('--log-probs', type=bool, default=False)
+    group.add_argument('--echo-prompts', type=bool, default=False)
+    group.add_argument('--batch', type=int, default=1)
+    group.add_argument('--model-name', type=str, default='my_llm')
+    return parser
+
+
+def check_args(args):
+    if args.temperature == 0.0:
+        args.top_p = 0.0
+        args.top_k = 1
+    assert args.temperature >= 0.0 and args.temperature <= 100.0, 'temperature must be a positive number less than or equal to 100.0'
+
+
+if __name__ == '__main__':
+    initialize_megatron(extra_args_provider=add_inference_args,
+                        args_defaults={
+                            'no_load_rng': True,
+                            'no_load_optim': True,
+                            'use_checkpoint_args': True
+                        })
+    args = get_args()
+    check_args(args)
+    # todo: support interleaved pipeline schedule
+    model = get_model(model_provider, wrap_with_ddp=False)
+    if args.load is not None:
+        _ = load_checkpoint(model, None, None)
+    assert len(model) == 1, "Load checkpoint failed"
+    model = model[0]
+    infer(model, args)
diff --git a/tools/run_text_generation_server.py b/tools/run_text_generation_server.py
index 5278915..f9757c8 100644
--- a/tools/run_text_generation_server.py
+++ b/tools/run_text_generation_server.py
@@ -5,7 +5,6 @@ import os
 import sys
 sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__),
                                              os.path.pardir)))
-import socket
 from megatron import get_args
 from megatron import print_rank_0
 from megatron.core import mpu
@@ -31,15 +30,8 @@ def model_provider(pre_process=True, post_process=True):
 
 def add_text_generate_args(parser):
     group = parser.add_argument_group(title='text generation')
-
-    group.add_argument("--temperature", type=float, default=1.0,
-                       help='Sampling temperature.')
-    group.add_argument("--top_p", type=float, default=0.0,
-                       help='Top p sampling.')
-    group.add_argument("--top_k", type=int, default=0,
-                       help='Top k sampling.')
-    group.add_argument("--out-seq-length", type=int, default=1024,
-                       help='Size of the output generated text.')
+    group.add_argument("--port", type=int, default=5000,
+                       help='Text generation server port.')
     return parser
 
 
@@ -66,7 +58,7 @@ if __name__ == "__main__":
     model = model[0]
     if mpu.is_pipeline_first_stage() and mpu.get_tensor_model_parallel_rank() == 0:
         server = MegatronServer(model)
-        server.run("0.0.0.0")
+        server.run("0.0.0.0", args.port)
 
     while True:
         choice = torch.cuda.LongTensor(1)
